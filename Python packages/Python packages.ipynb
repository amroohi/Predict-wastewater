{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\">    <b>Package:</b>    scikit learn</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn import metrics\n",
    "import hydroeval as he\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor      # KNN\n",
    "from sklearn.neural_network import MLPRegressor        # MLP\n",
    "from sklearn.svm import SVR                            # SVM\n",
    "from sklearn.tree import DecisionTreeRegressor         # DT\n",
    "from sklearn.ensemble import RandomForestRegressor     # RF\n",
    "from sklearn.ensemble import GradientBoostingRegressor # GBM\n",
    "from sklearn.ensemble import ExtraTreesRegressor       # ET\n",
    "\n",
    "# LSTM|\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.xlabel('Key variables')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "- Output variables are identified with the suffix _eff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "data = pd.read_excel(\"C:/Users/sh/Desktop/term 3 ut/Thesis and Paper/R/New folder/MWWTP_impute_data.xlsx\")\n",
    "data_missing = pd.read_excel(\"C:/Users/sh/Desktop/term 3 ut/Thesis and Paper/R/New folder/MWWTP_raw_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read column names of data and use it as input(x) and output(y) variables\n",
    "input_variables = data.columns[2:14]\n",
    "output_variables = data.columns[0:2]\n",
    "print(input_variables)\n",
    "print(output_variables)\n",
    "train_test_split_point = int(0.8*len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "excel_name = ['KNN_continuous_data.xlsx',\n",
    "              'MLP_continuous_data.xlsx',\n",
    "              'SVM_continuous_data.xlsx',\n",
    "              'DT_continuous_data.xlsx',\n",
    "              'RF_continuous_data.xlsx',\n",
    "              'GBM_continuous_data.xlsx',\n",
    "              'ET_continuous_data.xlsx',\n",
    "              'LSTM_continuous_data.xlsx']\n",
    "\n",
    "for name in excel_name:\n",
    "    workbook = xlsxwriter.Workbook(name)\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    \n",
    "    for i in range(0,len(output_variables)):\n",
    "        worksheet.write(''.join(string.ascii_uppercase[i+1] + '1'), output_variables[i])\n",
    "\n",
    "    worksheet.write('A2', 'RMSE_train')\n",
    "    worksheet.write('A3', 'RMSE_test')\n",
    "    worksheet.write('A4', 'R2_train')\n",
    "    worksheet.write('A5', 'R2_test')\n",
    "    worksheet.write('A6', 'NSE_train')\n",
    "    worksheet.write('A7', 'NSE_test')\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN \n",
    "(K nearest neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"KNN_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "column = 2\n",
    "# output_variables=['Q_eff']\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "    \n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()    \n",
    "    \n",
    "#     from sklearn.decomposition import PCA\n",
    "#     # Apply PCA to reduce the dimensionality of the data\n",
    "#     pca = PCA(n_components=10)\n",
    "#     x_train = pca.fit_transform(x_train)\n",
    "#     x_test = pca.transform(x_test) \n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)       \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    knn = KNeighborsRegressor()\n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"n_neighbors\": [20,25,30,35,40,45,50],\n",
    "                  \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "                  \"leaf_size\": [20,25,30,35,40,45,50,55],\n",
    "                  \"p\": [1,1.5,2,2.5,3]}\n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    knn_random = RandomizedSearchCV(estimator=knn,\n",
    "                                    param_distributions=param_dist,\n",
    "                                    cv=5,\n",
    "                                    n_iter=10)\n",
    "    knn_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", knn_random.best_params_)\n",
    "    print(\"Best Score: \", knn_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_knn = KNeighborsRegressor(n_neighbors=knn_random.best_params_['n_neighbors'],\n",
    "                                   algorithm=knn_random.best_params_['algorithm'],\n",
    "                                   leaf_size=knn_random.best_params_['leaf_size'],\n",
    "                                   p=knn_random.best_params_['p'])\n",
    "    best_knn.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_knn.predict(x_train_scaled)\n",
    "    predict_test = best_knn.predict(x_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "\n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_knn.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"KNN model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "    \n",
    "workbook.save(\"KNN_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "(Multi layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"MLP_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "column = 2\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "\n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)        \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    mlp = MLPRegressor()\n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"hidden_layer_sizes\": [(100,),(150,),(200,),(250,)],\n",
    "                  \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "                  \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "                  \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "                  \"learning_rate_init\": [0.001]}     \n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    mlp_random = RandomizedSearchCV(estimator=mlp,\n",
    "                                    param_distributions=param_dist,\n",
    "                                    cv=5,\n",
    "                                    n_iter=10)\n",
    "    mlp_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", mlp_random.best_params_)\n",
    "    print(\"Best Score: \", mlp_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_mlp = MLPRegressor(hidden_layer_sizes=mlp_random.best_params_['hidden_layer_sizes'],\n",
    "                            activation=mlp_random.best_params_['activation'],\n",
    "                            solver=mlp_random.best_params_['solver'],\n",
    "                            learning_rate=mlp_random.best_params_['learning_rate'],\n",
    "                            learning_rate_init=mlp_random.best_params_['learning_rate_init'])\n",
    "    best_mlp.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_mlp.predict(x_train_scaled)\n",
    "    predict_test = best_mlp.predict(x_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_mlp.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"MLP model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "    \n",
    "workbook.save(\"MLP_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "(Support-vector machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openpyxl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m workbook \u001b[38;5;241m=\u001b[39m \u001b[43mopenpyxl\u001b[49m\u001b[38;5;241m.\u001b[39mload_workbook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM_continuous_data.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m w_sheet \u001b[38;5;241m=\u001b[39m workbook[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSheet1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m df_plot \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(input_variables,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openpyxl' is not defined"
     ]
    }
   ],
   "source": [
    "workbook = openpyxl.load_workbook(\"SVM_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "\n",
    "column = 2\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "\n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)     \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    svm = SVR()\n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"tol\": [0.000001],\n",
    "                  \"degree\": [2,3,4,5],\n",
    "                  \"C\": [1,2,3],\n",
    "                  \"gamma\": [\"scale\",\"auto\"]}     \n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    svm_random = RandomizedSearchCV(estimator=svm,\n",
    "                                    param_distributions=param_dist,\n",
    "                                    cv=5,\n",
    "                                    n_iter=10)\n",
    "    svm_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", svm_random.best_params_)\n",
    "    print(\"Best Score: \", svm_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_svm = SVR(tol=svm_random.best_params_['tol'],\n",
    "                   C=svm_random.best_params_['C'],\n",
    "                   degree=svm_random.best_params_['degree'],\n",
    "                   gamma=svm_random.best_params_['gamma'])\n",
    "    best_svm.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_svm.predict(x_train_scaled)\n",
    "    predict_test = best_svm.predict(x_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_svm.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"SVM model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "    \n",
    "workbook.save(\"SVM_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT\n",
    "(Decision tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"DT_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "column = 2\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "\n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)        \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    DT = DecisionTreeRegressor()\n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"max_depth\": [3,5,10,15,20],\n",
    "                  \"min_samples_split\": [5,10,15,20,25,30,35,40],\n",
    "                  \"min_samples_leaf\": [50,100,150,200,250,300]}     \n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    DT_random = RandomizedSearchCV(estimator=DT,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   cv=5,\n",
    "                                   n_iter=10)\n",
    "    DT_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", DT_random.best_params_)\n",
    "    print(\"Best Score: \", DT_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_DT = DecisionTreeRegressor(max_depth=DT_random.best_params_['max_depth'],\n",
    "                                    min_samples_split=DT_random.best_params_['min_samples_split'],\n",
    "                                    min_samples_leaf=DT_random.best_params_['min_samples_leaf'])\n",
    "    best_DT.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "#     from sklearn import tree\n",
    "#     tree.plot_tree(best_DT)\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_DT.predict(x_train_scaled)\n",
    "    predict_test = best_DT.predict(x_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_DT.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"DT model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show()     \n",
    "\n",
    "workbook.save(\"DT_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF\n",
    "(Random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"RF_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "column = 2\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "\n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)         \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"max_depth\": [3,5,10,15,20],\n",
    "                  \"min_samples_split\": [2,5,10,15,20,25,30,35,40],\n",
    "                  \"min_samples_leaf\": [20,50,100,150,200,250,300]}     \n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    rf_random = RandomizedSearchCV(estimator=rf,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   cv=5,\n",
    "                                   n_iter=10)\n",
    "    rf_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", rf_random.best_params_)\n",
    "    print(\"Best Score: \", rf_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_rf = RandomForestRegressor(max_depth=rf_random.best_params_['max_depth'],\n",
    "                                    min_samples_split=rf_random.best_params_['min_samples_split'],\n",
    "                                    min_samples_leaf=rf_random.best_params_['min_samples_leaf'])\n",
    "    best_rf.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_rf.predict(x_train_scaled)\n",
    "    predict_test = best_rf.predict(x_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_rf.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"RF model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "    \n",
    "workbook.save(\"RF_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM\n",
    "(Gradient boosting model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"GBM_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "\n",
    "column = 2\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "\n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)         \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    gbm = GradientBoostingRegressor()\n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"n_estimators\": [50,100,150,200,250,300,350],\n",
    "                  \"min_samples_split\": [2,5,10,15,20,25,30,35,40],\n",
    "                  \"min_samples_leaf\": [20,50,100,150,200,250,300],\n",
    "                  \"learning_rate\": [0.01,0.1,0.5],\n",
    "                  \"max_depth\": [3,5,10,15,20]}     \n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    gbm_random = RandomizedSearchCV(estimator=gbm,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   cv=5,\n",
    "                                   n_iter=10)\n",
    "    gbm_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", gbm_random.best_params_)\n",
    "    print(\"Best Score: \", gbm_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_gbm = GradientBoostingRegressor(n_estimators=gbm_random.best_params_['n_estimators'],\n",
    "                                         min_samples_split=gbm_random.best_params_['min_samples_split'],\n",
    "                                         min_samples_leaf=gbm_random.best_params_['min_samples_leaf'],\n",
    "                                         learning_rate=gbm_random.best_params_['learning_rate'],\n",
    "                                         max_depth=gbm_random.best_params_['max_depth'])\n",
    "    best_gbm.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_gbm.predict(x_train_scaled)\n",
    "    predict_test = best_gbm.predict(x_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_gbm.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"GBM model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "    \n",
    "workbook.save(\"GBM_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET\n",
    "(extra-trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"ET_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "\n",
    "column = 2\n",
    "for target in output_variables:\n",
    "    \n",
    "    x = data[input_variables].to_numpy()\n",
    "    y = data[[target]].to_numpy()\n",
    "        \n",
    "    # splitting data into train and test\n",
    "    x_train = x[:train_test_split_point]\n",
    "    y_train = y[:train_test_split_point]    \n",
    "    x_test = data[input_variables][train_test_split_point:]\n",
    "    y_test = data[[target]][train_test_split_point:]\n",
    "\n",
    "    # drop target that is nan in test data\n",
    "    nan_index = data_missing[target].index[data_missing[target].apply(np.isnan)]\n",
    "    for nan in nan_index:\n",
    "        if nan>=train_test_split_point:\n",
    "            x_test = x_test.drop([nan])\n",
    "            y_test = y_test.drop([nan])\n",
    "            \n",
    "    x_test = x_test.to_numpy()        \n",
    "    y_test = y_test.to_numpy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    y_train_scaled = scaler.fit_transform(y_train)\n",
    "    x_test_scaled = scaler.fit_transform(x_test)\n",
    "    y_test_scaled = scaler.fit_transform(y_test)        \n",
    "\n",
    "    # Creating the Regression Model\n",
    "    ET = ExtraTreesRegressor()    \n",
    "    \n",
    "    # Defining the Range of Hyperparameters to Tune\n",
    "    param_dist = {\"max_depth\": [3,5,10,15,20],\n",
    "                  \"min_samples_split\": [2,5,10,15,20,25,30,35,40],\n",
    "                  \"min_samples_leaf\": [20,50,100,150,200,250,300],\n",
    "                  \"n_estimators\": [20,50,100,150,200,250,300]}     \n",
    "    \n",
    "    # Tuning Hyperparameters using Randomized Search Method\n",
    "    ET_random = RandomizedSearchCV(estimator=ET,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   cv=5,\n",
    "                                   n_iter=10)\n",
    "    ET_random.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    print(\"Best Hyperparameters: \", ET_random.best_params_)\n",
    "    print(\"Best Score: \", ET_random.best_score_)\n",
    "    \n",
    "    # Fitting the Model with Best Hyperparameters\n",
    "    best_ET = ExtraTreesRegressor(max_depth=ET_random.best_params_['max_depth'],\n",
    "                                  min_samples_split=ET_random.best_params_['min_samples_split'],\n",
    "                                  min_samples_leaf=ET_random.best_params_['min_samples_leaf'],\n",
    "                                  n_estimators=ET_random.best_params_['n_estimators'])\n",
    "    best_ET.fit(x_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "    # Predict target variable in train and test dataset\n",
    "    predict_train = best_ET.predict(x_train_scaled)\n",
    "    predict_test = best_ET.predict(x_test_scaled)\n",
    "        \n",
    "    # Calculate the accuracy of prediction model  \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))    \n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test < 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test,\n",
    "                                                  len(y_test)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train < 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train,\n",
    "                                                   len(y_train)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1\n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.vstack((y_train,y_test)), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)\n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train,y_test,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.AdditiveExplainer(best_ET.predict, x_train_scaled)\n",
    "    shap_values = explainer(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values, features=x_train_scaled, feature_names=data.columns[11:31], plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values)):\n",
    "            if shap_values[i][x].values>0:\n",
    "                n.append(shap_values[i][x].values)\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"ET model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "        \n",
    "        \n",
    "workbook.save(\"ET_continuous_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-block alert-info\">    <b>Package:</b>    Keras</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "(long-short term memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workbook = openpyxl.load_workbook(\"LSTM_continuous_data.xlsx\")\n",
    "w_sheet = workbook[\"Sheet1\"]\n",
    "\n",
    "df_plot = pd.DataFrame(input_variables,columns=['input'])\n",
    "column = 2\n",
    "# output_variables=[\"Q_eff\"]\n",
    "for target in output_variables:\n",
    "    \n",
    "    print(\"Target\", target)\n",
    "    # Split data into training and testing sets\n",
    "    train_data = data[:train_test_split_point]\n",
    "    test_data = data[train_test_split_point:]\n",
    "    \n",
    "    # Define input and output variables\n",
    "    x_train_scaled = train_data[input_variables].values\n",
    "    y_train_scaled = train_data[target].values\n",
    "    x_test_scaled = test_data[input_variables].values\n",
    "    y_test_scaled = test_data[target].values\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_scaled)\n",
    "    y_train_scaled = y_train_scaled.reshape(-1,1)\n",
    "    y_train_scaled = scaler.fit_transform(y_train_scaled)\n",
    "    x_test_scaled = scaler.fit_transform(x_test_scaled)\n",
    "    y_test_scaled = y_test_scaled.reshape(-1,1)\n",
    "    y_test_scaled = scaler.fit_transform(y_test_scaled) \n",
    "    \n",
    "    # Reshape input data for LSTM model\n",
    "    x_train_scaled = np.reshape(x_train_scaled, (x_train_scaled.shape[0], 1, x_train_scaled.shape[1]))\n",
    "    x_test_scaled = np.reshape(x_test_scaled, (x_test_scaled.shape[0], 1, x_test_scaled.shape[1]))\n",
    "\n",
    "    # Define LSTM model\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(16, input_shape=(1, len(input_variables)), activation='relu'))\n",
    "    lstm.add(Dense(2, kernel_initializer='normal', activation='linear'))\n",
    "    lstm.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    lstm.summary()\n",
    "    \n",
    "    # Fit model to training data\n",
    "    lstm.fit(x_train_scaled, y_train_scaled, epochs=500, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Make predictions on testing data\n",
    "    predict_train = lstm.predict(x_train_scaled)\n",
    "    predict_test = lstm.predict(x_test_scaled)\n",
    "\n",
    "    # Calculate the accuracy of prediction model \n",
    "    predict_train = scaler.inverse_transform(predict_train.reshape(-1, 1))\n",
    "    predict_test = scaler.inverse_transform(predict_test.reshape(-1, 1))\n",
    "    y_train_scaled = scaler.inverse_transform(y_train_scaled.reshape(-1, 1))\n",
    "    y_test_scaled = scaler.inverse_transform(y_test_scaled.reshape(-1, 1))\n",
    "    \n",
    "    nse_train = he.evaluator(he.nse,\n",
    "                             y_train_scaled,\n",
    "                             predict_train)\n",
    "    nse_test  = he.evaluator(he.nse,\n",
    "                             y_test_scaled,\n",
    "                             predict_test)\n",
    "    \n",
    "    R2_train = metrics.r2_score(y_train_scaled,\n",
    "                                predict_train)\n",
    "    R2_test  = metrics.r2_score(y_test_scaled,\n",
    "                                predict_test)\n",
    "    \n",
    "    if R2_test <= 0:\n",
    "        corr_matrix_test = np.corrcoef(np.reshape(y_test_scaled,\n",
    "                                                  len(y_test_scaled)),\n",
    "                                       np.reshape(predict_test,\n",
    "                                                  len(predict_test)))\n",
    "        R2_test = corr_matrix_test[0,1]**2\n",
    "        \n",
    "    if R2_train <= 0:\n",
    "        corr_matrix_train = np.corrcoef(np.reshape(y_train_scaled,\n",
    "                                                   len(y_train_scaled)),\n",
    "                                        np.reshape(predict_train,\n",
    "                                                   len(predict_train)))\n",
    "        R2_train = corr_matrix_train[0,1]**2\n",
    "    \n",
    "    RMSE_train = np.sqrt(((y_train_scaled - predict_train) ** 2).mean())\n",
    "    RMSE_test = np.sqrt(((y_test_scaled - predict_test) ** 2).mean())\n",
    "    \n",
    "    # write on excel file\n",
    "    row = 2\n",
    "    indices = [round(RMSE_train,2),\n",
    "               round(RMSE_test,2),\n",
    "               round(R2_train,2),\n",
    "               round(R2_test,2),\n",
    "               round(nse_train[0],2),\n",
    "               round(nse_test[0],2)]\n",
    "    print(R2_test)\n",
    "    \n",
    "    # iterating through indices list\n",
    "    for item in indices:\n",
    "        w_sheet.cell(row=row, column=column).value = item\n",
    "        row += 1\n",
    "               \n",
    "    column += 1 \n",
    "    \n",
    "    # Plot the predicted values and true values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.margins(x=0,y=0)\n",
    "    plt.plot(np.concatenate((y_train_scaled, y_test_scaled), axis=0), linestyle='-', color='blue', linewidth=0.5)\n",
    "    plt.plot(np.vstack((predict_train,predict_test)), linestyle='-', color='black', linewidth=0.5)   \n",
    "    plt.fill_between((train_test_split_point, len(data)),\n",
    "                     min(np.vstack((y_train_scaled,y_test_scaled,predict_train,predict_test))),\n",
    "                     max(np.vstack((y_train_scaled,y_test_scaled,predict_train,predict_test))),\n",
    "                     facecolor='black', alpha = 0.1)    \n",
    "    plt.show()\n",
    "        \n",
    "    # explain\n",
    "    explainer = shap.GradientExplainer(lstm, x_train_scaled)\n",
    "    shap_values = explainer.shap_values(x_train_scaled)\n",
    "#     shap.summary_plot(shap_values[0][:, 0, :], feature_names=data.columns[11:31], plot_size=0.2, plot_type=\"bar\")\n",
    "\n",
    "    m=[]\n",
    "    for x in range(0,len(input_variables)):\n",
    "        n=[]\n",
    "        for i in range(0,len(shap_values[0][:, 0, :])):\n",
    "            if shap_values[0][:, 0, :][i][x]>0:\n",
    "                n.append(shap_values[0][:, 0, :][i][x])\n",
    "        m.append(np.average(n))\n",
    "        \n",
    "    df_plot[target] = m\n",
    "\n",
    "colors = plt.cm.Greys(np.linspace(0.4, 0.8, 2))\n",
    "df_plot.plot(x='input', kind='bar', stacked=True, color=colors)\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"LSTM model\")\n",
    "plt.ylabel('mean(|SHAP value|) (average impact on model output)')\n",
    "plt.xlabel('input variables')\n",
    "plt.show() \n",
    "    \n",
    "workbook.save(\"LSTM_continuous_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
